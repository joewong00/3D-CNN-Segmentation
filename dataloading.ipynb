{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from dataloader import MRIDataset\n",
    "from residual3dunet.model import ResidualUNet3D, UNet3D\n",
    "# from unet3d.model import UNet3D\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_loaders\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "from elastic_transform import RandomElastic\n",
    "import nibabel as nib\n",
    "#from residual3dunet.modelorig import ResidualUNet3D\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MRIDataset(train=True)\n",
    "train, val = torch.utils.data.random_split(dataset, [40, 10])\n",
    "\n",
    "dataloader = DataLoader(dataset = train, batch_size= 1, shuffle= True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MRIDataset(train=True, transform=True)\n",
    "\n",
    "train_kwargs = {'batch_size': 10}\n",
    "cuda_kwargs = {'num_workers': 1, 'pin_memory': True,'shuffle': True}\n",
    "train_kwargs.update(cuda_kwargs)\n",
    "\n",
    "#train, val = random_split(dataset, [40, 10])\n",
    "# dataloader = DataLoader(dataset = val, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, valloader = get_loaders(train=True, transform=True, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)\n",
    "data = dataiter.next()\n",
    "features, labels = data\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,1,14,240,240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ResidualUNet3D(in_channels=1, out_channels=1, f_maps=64, num_levels=4)\n",
    "model2 = UNet3D(in_channels=1, out_channels=1, f_maps=64, num_levels=4)\n",
    "# print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channel=1, out_channel=1, training=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.training = training\n",
    "        self.encoder1 = nn.Sequential(\n",
    "                nn.GroupNorm(num_groups=1, num_channels=in_channel),\n",
    "                nn.Conv3d(in_channel, 32, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.GroupNorm(num_groups=8, num_channels=32),\n",
    "                nn.Conv3d(32, 64, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True)\n",
    "            )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "                nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "                nn.Conv3d(64, 64, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "                nn.Conv3d(64, 128, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True)\n",
    "            )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "                nn.GroupNorm(num_groups=8, num_channels=128),\n",
    "                nn.Conv3d(128, 128, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.GroupNorm(num_groups=8, num_channels=128),\n",
    "                nn.Conv3d(128, 256, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True)\n",
    "            )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "                nn.GroupNorm(num_groups=8, num_channels=256),\n",
    "                nn.Conv3d(256, 256, 3, padding=1, bias=False),\n",
    "                # nn.ELU(inplace=True),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.GroupNorm(num_groups=8, num_channels=256),\n",
    "                nn.Conv3d(256, 512, 3, padding=1, bias=False),\n",
    "                # nn.ELU(inplace=True),\n",
    "                nn.ELU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.decoder2 = nn.Sequential(\n",
    "                nn.GroupNorm(num_groups=8, num_channels=768),\n",
    "                nn.Conv3d(768, 256, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.GroupNorm(num_groups=8, num_channels=256),\n",
    "                nn.Conv3d(256, 256, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.decoder3 = nn.Sequential(\n",
    "                nn.GroupNorm(num_groups=8, num_channels=384),\n",
    "                nn.Conv3d(384, 128, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.GroupNorm(num_groups=8, num_channels=128),\n",
    "                nn.Conv3d(128, 128, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.decoder4 = nn.Sequential(\n",
    "                nn.GroupNorm(num_groups=8, num_channels=192),\n",
    "                nn.Conv3d(192, 64, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True),\n",
    "                nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "                nn.Conv3d(64, 64, 3, padding=1, bias=False),\n",
    "                nn.ELU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.decoder5 = nn.Conv3d(64, 1, 1)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(1,2,2))\n",
    "        self.upsampling = InterpolateUpsampling('trilinear')\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        print(x.shape)\n",
    "        x1 = self.encoder1(x) # 1,64,14,240,240\n",
    "        print(x1.shape)\n",
    "\n",
    "        x2 = self.encoder2(self.maxpool(x1)) # 1,128,14,120,120\n",
    "        print(x2.shape)\n",
    "\n",
    "        x3 = self.encoder3(self.maxpool(x2)) # 1,256,14,60,60\n",
    "        print(x3.shape)\n",
    "\n",
    "        x4 = self.encoder4(self.maxpool(x3)) # 1,512,14,30,30\n",
    "        print(x4.shape)   \n",
    "\n",
    "        out = x3.size()[2:]\n",
    "        x5 = torch.cat((x3, self.upsampling(x4, out)),dim=1)\n",
    "        x5 = self.decoder2(x5)\n",
    "        print(x5.shape)\n",
    "\n",
    "        out = x2.size()[2:]\n",
    "        x6 = torch.cat((x2, self.upsampling(x5, out)),dim=1)\n",
    "        x6 = self.decoder3(x6)\n",
    "        print(x6.shape)\n",
    "\n",
    "        out = x1.size()[2:]\n",
    "        x7 = torch.cat((x1, self.upsampling(x6, out)),dim=1)\n",
    "        x7 = self.decoder4(x7)\n",
    "        print(x7.shape)\n",
    "\n",
    "        out = self.decoder5(x7)\n",
    "        print(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class InterpolateUpsampling(nn.Module):\n",
    "    def __init__(self, mode):\n",
    "        super(InterpolateUpsampling, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x, size):\n",
    "        return F.interpolate(x,size=size, mode=self.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 14, 240, 240])\n",
      "torch.Size([1, 64, 14, 240, 240])\n",
      "torch.Size([1, 128, 14, 120, 120])\n",
      "torch.Size([1, 256, 14, 60, 60])\n",
      "torch.Size([1, 512, 14, 30, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joewong/miniforge3/envs/rosetta/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 14, 60, 60])\n",
      "torch.Size([1, 128, 14, 120, 120])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported memory format. Supports only ChannelsLast, Contiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/21/q6hd2x45603c6rqjbyj5tsx00000gn/T/ipykernel_19877/2265758082.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/rosetta/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/21/q6hd2x45603c6rqjbyj5tsx00000gn/T/ipykernel_19877/2395175910.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mx7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mx7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rosetta/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rosetta/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rosetta/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rosetta/lib/python3.7/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         return F.group_norm(\n\u001b[0;32m--> 269\u001b[0;31m             input, self.num_groups, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/rosetta/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m     \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsupported memory format. Supports only ChannelsLast, Contiguous"
     ]
    }
   ],
   "source": [
    "model = UNet(1,1)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = MRIDataset(train=True, transform=T.Compose([\n",
    "    T.ToTensor(), \n",
    "    T.RandomHorizontalFlip(), \n",
    "    T.RandomCrop((240,240), padding=50, pad_if_needed=True),\n",
    "    ]), elastic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loader = DataLoader(dataset = dataset2, batch_size=50, shuffle=False)\n",
    "\n",
    "# dataiter = iter(test_loader)\n",
    "\n",
    "# first = next(dataiter)\n",
    "# second = next(dataiter)\n",
    "\n",
    "# features1, labels1 = first\n",
    "# features2, labels2 = second\n",
    "\n",
    "for data, target in test_loader:\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5ftrain = h5py.File('dataset/T2train.h5','r')\n",
    "# h5ftrainmask = h5py.File('dataset/T2trainmask.h5','r')\n",
    "\n",
    "# data = h5ftrain[f'T2data_2'][:]\n",
    "# target = h5ftrainmask[f'T2maskdata_2'][:]\n",
    "\n",
    "# print(data.shape)\n",
    "# print(target.shape)\n",
    "\n",
    "# image_path = './dataset/train/T1/MRI2_T1.nii.gz'\n",
    "# image_obj = nib.load(image_path)\n",
    "# # print(f'Type of the image {type(image_obj)}')\n",
    "\n",
    "# # Extract data as numpy array\n",
    "# image_data = image_obj.get_fdata()\n",
    "# print(type(image_data))\n",
    "# print(image_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "# image_data = np.pad(image_data, ((0,0),(0,0),(0,1)))\n",
    "# image_data = np.moveaxis(image_data, 2, 0)\n",
    "# image_data = np.moveaxis(image_data, 2, 1)\n",
    "# image_data = torch.from_numpy(image_data)\n",
    "# image_data = torch.unsqueeze(image_data, 0)\n",
    "# print(image_data.shape)\n",
    "\n",
    "# preprocess = RandomElastic(alpha=0, sigma=0.06)\n",
    "# data1, target1 = preprocess(data, target)\n",
    "\n",
    "# print(data1.shape)\n",
    "# print(target1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = random.randint(0,49)\n",
    "def explore_3d_image(layer):\n",
    "\n",
    "    plt.figure(figsize=(5,10))\n",
    "    # plt.imshow(data[0,layer,:,:],cmap='gray')\n",
    "    \n",
    "    plt.imshow(t1data[layer,:,:],cmap='gray')\n",
    "    plt.imshow(t2mask[layer,:,:],cmap='gray', alpha=0.3)\n",
    "    plt.title('Explore Layers of Kidney MRI')\n",
    "    plt.axis('off')\n",
    "    return layer\n",
    "\n",
    "interact(explore_3d_image, layer=(0,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7aa30a5429a76fc9327cf5c7d6cbac98b63911354387ecfa6ca386d1236b51ba"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
