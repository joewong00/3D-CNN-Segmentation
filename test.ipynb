{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from dataloader import MRIDataset\n",
    "# from residual3dunet.model import ResidualUNet3D\n",
    "from residual3dunet.res3dunetmodel import ResidualUNet3D\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import dice_coefficient, compute_average, add_mask_colour\n",
    "import torchvision.transforms as T\n",
    "import nibabel as nib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotloss('output/train16.out')\n",
    "plt.figure()\n",
    "# plotaccuracy('output/train6.out')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model = ResidualUNet3D(in_channels=1, out_channels=1, testing=True).to(device)\n",
    "\n",
    "# model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(\"model28.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset = MRIDataset(train=False, transform=T.ToTensor())\n",
    "test_loader = DataLoader(dataset = testdataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(len(test_loader))\n",
    "\n",
    "image_path = './dataset/train/T1/MRI1_T1.nii.gz'\n",
    "image_obj = nib.load(image_path)\n",
    "\n",
    "# Extract data as numpy array\n",
    "image_data = image_obj.get_fdata()\n",
    "\n",
    "image_data = np.pad(image_data, ((0,0),(0,0),(0,1)))\n",
    "image_data = np.moveaxis(image_data, 2, 0)\n",
    "image_data = np.moveaxis(image_data, 2, 1)\n",
    "image_data = torch.from_numpy(image_data)\n",
    "image_data = torch.unsqueeze(image_data, 0)\n",
    "image_data = torch.unsqueeze(image_data, 0)\n",
    "# print(image_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "data = dataiter.next()\n",
    "# data = dataiter.next()\n",
    "# data = dataiter.next()\n",
    "features, labels = data\n",
    "\n",
    "features, labels = features.float().to(device), labels.float().to(device)\n",
    "\n",
    "# image_data = image_data.float().to(device)\n",
    "\n",
    "output = model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = torch.sigmoid(output)\n",
    "preds = (output > 0.5).float()\n",
    "\n",
    "# print(preds.shape)\n",
    "# print(labels.shape)\n",
    "batch, channel, depth, width, height = preds.shape\n",
    "\n",
    "print(dice_coefficient(preds, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Single Image Data\n",
    "f, axarr = plt.subplots(14,3,figsize=(50,50))\n",
    "\n",
    "for i in range(depth):\n",
    "    axarr[i,0].imshow(features[0,0,i,:,:],cmap='gray')\n",
    "    axarr[i,1].imshow(preds[0,0,i,:,:],cmap='gray')\n",
    "    axarr[i,2].imshow(labels[0,0,i,:,:],cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_statistics import SegmentationStatistics\n",
    "\n",
    "preds = preds.numpy()\n",
    "labels = labels.numpy()\n",
    "prediction = preds.astype(bool)\n",
    "gt = labels.astype(bool)\n",
    "\n",
    "# preds = preds.bool()\n",
    "# labels = labels.bool()\n",
    "\n",
    "stat = SegmentationStatistics(prediction[0,0,:,:,:], gt[0,0,:,:,:], (3,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "features.shape\n",
    "\n",
    "# labels = add_mask_colour(labels, \"blue\")\n",
    "# preds = add_mask_colour(preds, \"red\")\n",
    "# overlap = labels + preds\n",
    "\n",
    "# print(labels.shape)\n",
    "\n",
    "def explore_3d_image(layer):\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(features[0,0,layer,:,:],cmap='gray')\n",
    "    # plt.imshow(overlap[layer,:,:,:], alpha=0.4)\n",
    "    plt.title('Explore Segmented MRI')\n",
    "    plt.axis('off')\n",
    "    # return layer\n",
    "\n",
    "interact(explore_3d_image,layer=(0,features.shape[2]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(14,2,figsize=(100,100))\n",
    "\n",
    "for i in range(depth):\n",
    "    axarr[i,0].imshow(features[0,0,i,:,:],cmap='gray')\n",
    "    axarr[i,1].imshow(overlap[i,:,:,:],cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7aa30a5429a76fc9327cf5c7d6cbac98b63911354387ecfa6ca386d1236b51ba"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
